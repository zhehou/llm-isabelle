0) One-time prep (env + tiny goals)
# Point to Ollama & pick a default model
export OLLAMA_HOST="http://127.0.0.1:11434"
export OLLAMA_MODEL="qwen3-coder:30b"

# Keep logs local
export LOG_DIR=".logs"
mkdir -p "$LOG_DIR" benchmarks/results

# Create tiny ad-hoc goals file for quick smoke tests
cat > /tmp/sanity_goals.txt << 'EOF'
rev (rev xs) = xs
map f (xs @ ys) = map f xs @ map f ys
n + 0 = n
∀x. x ∈ A ∩ B ⟷ x ∈ A ∧ x ∈ B
EOF

1) Quick single-goal proof (CLI)
python -m prover.cli --goal 'rev (rev xs) = xs' --beam 2 --max-depth 6 --budget-s 8 --trace


Exercises core loop, beam/ depth/ budget flags, and trace printing.

2) Run a file of goals (CLI)
python -m prover.cli --goals-file /tmp/sanity_goals.txt --beam 2 --max-depth 6 --budget-s 6


Verifies batch mode via --goals-file. Logs go to .logs/attempts.log.jsonl and .logs/runs.log.jsonl.

3) Train the reranker (scikit-learn) and enable it
# Collect some attempts first (run #2 or #1 a few times)
python -m prover.train_reranker_sklearn
# Expect a model at .models/sk_reranker.joblib


Uses attempts log → feature extraction → saves sk_reranker.joblib. The runtime loads it automatically if present.

Test with reranker on (default):

python -m prover.cli --goal 'map f (xs @ ys) = map f xs @ map f ys' --beam 3 --max-depth 6 --budget-s 8


Disable reranker via flag/env:

python -m prover.cli --goal 'map f (xs @ ys) = map f xs @ map f ys' --beam 3 --max-depth 6 --budget-s 8 --no-reranker
# or: export RERANKER_DISABLE=1


The CLI flag --no-reranker and env RERANKER_DISABLE are honored by the proving loop.

4) Train the reranker (XGBoost) and hot-swap
python -m prover.train_reranker_xgb
# Reuses same .models/sk_reranker.joblib path so nothing else changes


Confirms the XGBoost pipeline produces a compatible artifact.

5) Macros mining (2-step continuations) + env knobs
# Mine from existing runs.log.jsonl and use results at runtime automatically
python -m prover.cli --goal 'n + 0 = n' --beam 2 --max-depth 4 --budget-s 6

# Tighten macro mining thresholds (optional; used when macros are mined)
export MACRO_MIN_COUNT=5
export MACRO_MAX_PER_HEAD=3
python -m prover.cli --goal 'map f (xs @ ys) = map f xs @ map f ys' --beam 2 --max-depth 5 --budget-s 6


Tests the macro feature and the env-plumbed tunables. (Macros are consumed inside cli → prover.prove_goal.)

6) Sledgehammer integration
python -m prover.cli --goal '∀xs. rev (rev xs) = xs' --sledge --sledge-timeout 5 --sledge-every 2 --beam 2 --max-depth 6 --budget-s 10


Ensures sledge suggestions are fetched & tried.

7) Quickcheck/ Nitpick pre-checks
# Quickcheck only
python -m prover.cli --goal 'rev xs = xs' --quickcheck --quickcheck-timeout 2 --beam 2 --max-depth 4 --budget-s 6

# Nitpick only
python -m prover.cli --goal 'rev xs = xs' --nitpick --nitpick-timeout 5 --beam 2 --max-depth 4 --budget-s 6

# Both
python -m prover.cli --goal 'rev xs = xs' --quickcheck --nitpick --beam 2 --max-depth 4 --budget-s 6


Verifies counterexample pruning paths.

8) Variants (structured proofs) + Minimization
# Enable structured proof search & minimizer
python -m prover.cli --goal 'rev (rev xs) = xs' --variants --no-minimize --beam 2 --max-depth 6 --budget-s 10 --trace


Should try converting a found proof to a structured Isar variant, then minimize facts/steps.

9) Model selection: single vs ensemble
# Single model (override env)
python -m prover.cli --goal 'map f (xs @ ys) = map f xs @ map f ys' --model 'qwen3-coder:30b'

# Ensemble
python -m prover.cli --goal 'map f (xs @ ys) = map f xs @ map f ys' --models 'qwen3-coder:30b,llama3.1:8b-instruct'


Confirms CLI → prover passes through models list.

10) Benchmark harness (suites, reranker & sledge matrix)
# Fast pass on one suite
python -m prover.bench --suite lists --beam 2 --max-depth 5 --budget-s 6 --reranker both --sledge off

# All built-ins (may take longer)
python -m prover.bench --suite all --beam 2 --max-depth 5 --budget-s 6 --reranker both --sledge off --shuffle

# Aggregate benchmarkign results
python -m prover.aggregate

Writes per-goal rows and a CSV summary into benchmarks/results/…. Confirms suite wiring and CSV output.

11) Regression testing and comparison with baseline
# Record a baseline for lists
python -m prover.regress --suite lists --beam 2 --max-depth 6 --budget-s 8 --save-baseline benchmarks/baselines/lists.json

# Later, detect regressions vs baseline (exit code 1 on regression)
python -m prover.regress --suite lists --baseline benchmarks/baselines/lists.json

# Just print a summary (no baseline)
python -m prover.regress --suite lists
